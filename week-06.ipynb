{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "## Introduction\n",
    "\n",
    "To begin, we're going to initialize a pandas DataFrame of Greek tragedy by line.\n",
    "\n",
    "You might be wondering why we aren't using Pausanias as usual. Tragedy has some nice built-in features that let us get to the heart of some experiments more quickly: rather than having broad geographical labels, each line comes pre-labeled by `dramatist`, `play`, and `speaker`.\n",
    "\n",
    "From the `speaker` label, we can derive information such as age, gender, social status, etc.\n",
    "\n",
    "These labels thus provide a lot of categorical data essentially for free, giving us a number of variables with which to experiment.\n",
    "\n",
    "I've gone ahead and pre-processed the Perseus XML for you, although you should take a look at [`preprocess.py`](./preprocess.py) when you have a chance so that you know what's happening.\n",
    "\n",
    "Let's load up the dataframe and confirm that things look as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_pickle(\"./greek-tragedy-by-line.pickle\")\n",
    "\n",
    "df[df['speaker'].str.lower() == 'chorus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Before we get into the details of TF-IDF --- a somewhat old-school method that still deserves attention --- let's get a feel for what its results look like.\n",
    "\n",
    "We'll divide the lines by play and collapse all of the rows per play into three very long strings. These strings are the **documents** that make up our **corpus**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.groupby(['dramatist', 'title'])['text'].apply(' '.join).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we're going to use the `TfidfVectorizer` class from `scikit-learn` to calculate the TF-IDF scores for each term in the corpus, labeled by document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(docs['text'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    index=docs['title'],\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    ")\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have only 31 rows but 12,121 columns -- a bit unwieldy. Let's pick a selection of words to examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    \"apollo\",\n",
    "    \"death\", \n",
    "    \"delphi\",\n",
    "    \"divinity\",\n",
    "    \"gods\",\n",
    "    \"humankind\",\n",
    "    \"humans\",\n",
    "    \"life\",\n",
    "    \"men\",\n",
    "    \"sanctuary\",\n",
    "    \"women\",\n",
    "    \"zeus\"\n",
    "]\n",
    "\n",
    "tfidf_df[keywords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not so bad. But what do these numbers tell us?\n",
    "\n",
    "> Discuss: How do you interpret the numbers above? What are some of the drawbacks of performing this analysis on translations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)\n",
    "\n",
    "We have already worked with **term frequency** extensively in this course: in its simplest form, it is the raw frequency of a given **term** in the **corpus**.\n",
    "\n",
    "But we can intuitively see that this simple form will have a bias towards longer documents: the more terms in a document, the more chances there are for any given term to occur.\n",
    "\n",
    "What is a **term** in this case? It could be a word, a token, a lemma, or n-grams thereof, or it could be a lexico-grammatical or syntactic feature. We use **term** as a flexible catch-all for any countable feature of the corpus.\n",
    "\n",
    "There are various ways of normalizing TF depending on the needs of our corpus and analysis. A common normalization is simply to convert the raw count for a term into a relative count -- we've done this in nearly every class, taking the absolute frequency of term `t` and dividing it by the total number of terms in a document.\n",
    "\n",
    "```math\n",
    "tf(t, d) = \\frac{f(t, d)}{\\sum_{t' \\in d}f(t', d)}\n",
    "```\n",
    "\n",
    "Alternatively, we can apply log normalization: \n",
    "\n",
    "```math\n",
    "tf(t, d) = \\log(1 + f(t, d))\n",
    "```\n",
    "\n",
    "Or we can even normalize according to the most frequently occurring term in the document:\n",
    "\n",
    "```math\n",
    "tf(t, d) = 0.5 + 0.5 \\cdot \\frac{f(t, d)}{max_{\\{t' \\in d\\}}f(t', d)}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Inverse) Document Frequency (DF)\n",
    "\n",
    "**Document frequency** is the number of documents containing the term divided by the total number of documents.\n",
    "\n",
    "**Inverse document frequency**, then, is the inverse of that function:\n",
    "\n",
    "```math\n",
    "idf(t, D) = \\log{\\frac{N}{|{d : d \\in D \\text{ and } t \\in d}|}}\n",
    "```\n",
    "\n",
    "Similarly $`tf(t, d)`$, $`idf(t, D)`$ can be weighted in various ways. \n",
    "\n",
    "> Discuss: For example, how might we account for a term that isn't in the corpus?\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting for notes on Scikit's implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "TF-IDF, then, is just the product of these two calculations.\n",
    "\n",
    "```math\n",
    "tfidf(t, d, D) = tf(t, d) \\cdot idf(t, D)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Write your own TF and IDF functions using techniques we have covered in the course so far.\n",
    "2. Use the \"documents\" that we passed to the `TfidfVectorizer` above, calculate your own scores for the English translations of the tragedies.\n",
    "3. Compare them to the scores from scikit-learn -- what might account for similarities and differences? (You can consult the documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF for document clustering\n",
    "\n",
    "So what can we use TF-IDF for? In addition to highlighting important terms for a given document, it can also be used for clustering documents and analyzing their overlap.\n",
    "\n",
    "We're going to follow [this tutorial](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-pyy) for clustering with \"sparse features,\" with modifications for our own (much smaller) dataset.\n",
    "\n",
    "\"Sparse features\" refers to our use of TF-IDF for labeling words in this case -- rather than a dense representation of words in the vocabulary of the corpus (e.g., word embeddings), we're clustering via the sparse feature of a TF-IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"talk.religion.misc\",\n",
    "    \"comp.graphics\",\n",
    "    \"sci.space\",\n",
    "]\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\",\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode(\"utf-8\")) for s in docs) / 1e6\n",
    "\n",
    "\n",
    "def load_dataset(verbose=False):\n",
    "    data_train, data_test = train_test_split(docs, test_size=0.4)\n",
    "\n",
    "    target_names = data_train.dramatist.unique()\n",
    "\n",
    "    # split target in a training set and a test set\n",
    "    y_train, y_test = data_train.dramatist, data_test.dramatist\n",
    "\n",
    "    # Extracting features from the training data using a sparse vectorizer\n",
    "    t0 = time()\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    X_train = vectorizer.fit_transform(data_train['text'])\n",
    "    duration_train = time() - t0\n",
    "\n",
    "    # Extracting features from the test data using the same vectorizer\n",
    "    t0 = time()\n",
    "    X_test = vectorizer.transform(data_test['text'])\n",
    "    duration_test = time() - t0\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    if verbose:\n",
    "        # compute size of loaded data\n",
    "        data_train_size_mb = size_mb(data_train['text'])\n",
    "        data_test_size_mb = size_mb(data_test['text'])\n",
    "\n",
    "        print(\n",
    "            f\"{len(data_train['text'])} documents - \"\n",
    "            f\"{data_train_size_mb:.2f}MB (training set)\"\n",
    "        )\n",
    "        print(f\"{len(data_test['text'])} documents - {data_test_size_mb:.2f}MB (test set)\")\n",
    "        print(f\"{len(target_names)} categories\")\n",
    "        print(\n",
    "            f\"vectorize training done in {duration_train:.3f}s \"\n",
    "            f\"at {data_train_size_mb / duration_train:.3f}MB/s\"\n",
    "        )\n",
    "        print(f\"n_samples: {X_train.shape[0]}, n_features: {X_train.shape[1]}\")\n",
    "        print(\n",
    "            f\"vectorize testing done in {duration_test:.3f}s \"\n",
    "            f\"at {data_test_size_mb / duration_test:.3f}MB/s\"\n",
    "        )\n",
    "        print(f\"n_samples: {X_test.shape[0]}, n_features: {X_test.shape[1]}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, feature_names, target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, feature_names, target_names = load_dataset(\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, pred, ax=ax)\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "_ = ax.set_title(\n",
    "    f\"Confusion Matrix for {clf.__class__.__name__}\\non the original documents\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try re-running the above analyses a few times. What do you notice? What does this mean about our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Rewrite the `load_data` function to train on play titles rather than dramatists. (Hint: You will need to rerun the `.groupby` operation above so that `title` is the left-most column in the DataFrame.)\n",
    "2. Rerun and analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "Reades, Jonathan, and Jennie Williams. 2023. “Clustering and Visualising Documents Using Word Embeddings.” Programming Historian, August. https://programminghistorian.org/en/lessons/clustering-visualizing-word-embeddings.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-pyy\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/linear-regression\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/logistic-regression\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
